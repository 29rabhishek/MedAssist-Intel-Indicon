{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neural-compressor\n",
      "  Downloading neural_compressor-3.1.1-py3-none-any.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting deprecated>=1.2.13\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (1.3.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (9.5.0)\n",
      "Collecting prettytable\n",
      "  Downloading prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (5.9.3)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy<2.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (1.18.5)\n",
      "Collecting pycocotools-windows\n",
      "  Downloading pycocotools_windows-2.0.0.2-cp37-cp37m-win_amd64.whl (83 kB)\n",
      "     ---------------------------------------- 83.4/83.4 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "     ---------------------------------------- 38.8/38.8 MB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from neural-compressor) (2.31.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from deprecated>=1.2.13->neural-compressor) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from pandas->neural-compressor) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from pandas->neural-compressor) (2024.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from prettytable->neural-compressor) (0.2.10)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from prettytable->neural-compressor) (6.7.0)\n",
      "Requirement already satisfied: setuptools>=18.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from pycocotools-windows->neural-compressor) (65.6.3)\n",
      "Collecting cython>=0.27.3\n",
      "  Downloading Cython-3.0.11-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 11.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from pycocotools-windows->neural-compressor) (3.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from requests->neural-compressor) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from requests->neural-compressor) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from requests->neural-compressor) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from requests->neural-compressor) (2.0.7)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from scikit-learn->neural-compressor) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from scikit-learn->neural-compressor) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools-windows->neural-compressor) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools-windows->neural-compressor) (3.1.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools-windows->neural-compressor) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools-windows->neural-compressor) (23.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools-windows->neural-compressor) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->neural-compressor) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from importlib-metadata->prettytable->neural-compressor) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages (from importlib-metadata->prettytable->neural-compressor) (3.15.0)\n",
      "Installing collected packages: schema, py-cpuinfo, threadpoolctl, opencv-python-headless, deprecated, cython, scikit-learn, prettytable, pycocotools-windows, neural-compressor\n",
      "Successfully installed cython-3.0.11 deprecated-1.2.15 neural-compressor-3.1.1 opencv-python-headless-4.10.0.84 prettytable-3.7.0 py-cpuinfo-9.0.0 pycocotools-windows-2.0.0.2 schema-0.7.7 scikit-learn-1.0.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neural-compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/s2304/.cache/huggingface/datasets/json/default-a82d8c98b11e56df/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Found cached dataset json (C:/Users/s2304/.cache/huggingface/datasets/json/default-a82d8c98b11e56df/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of './trained_trocr_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './trained_trocr_model' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m                     \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m                 )\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"repo_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"from_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"to_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    164\u001b[0m         raise HFValidationError(\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[1;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m             \u001b[1;34m\" forbidden, '-' and '.' cannot start or end the name, max length is 96:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './trained_trocr_model'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14312\\2595622889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mneural_compressor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mquantization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVisionEncoderDecoderModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./trained_trocr_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mquantized_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalib_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalib_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_fast_init\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2316\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2317\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2318\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2319\u001b[0m             )\n\u001b[0;32m   2320\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"foo\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         ```\"\"\"\n\u001b[1;32m--> 547\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             logger.warning(\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\s2304\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m                 \u001b[1;31m# For any other exception, we throw a generic error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                 raise EnvironmentError(\n\u001b[1;32m--> 651\u001b[1;33m                     \u001b[1;34mf\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m                     \u001b[1;34m\" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m                     \u001b[1;34mf\" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load the configuration of './trained_trocr_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './trained_trocr_model' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "conf = PostTrainingQuantConfig(\n",
    "    quant_level=\"auto\",  # Automatically adjust between conservative and aggressive quantization.\n",
    "    tuning_criterion=TuningCriterion(\n",
    "        strategy=\"basic\",  # Can be \"mse\" or \"basic\" for transformer models.\n",
    "        timeout=0,  # For early stopping if the accuracy loss is intolerable.\n",
    "        max_trials=32  # Maximum tuning trials.\n",
    "    )\n",
    ")\n",
    "calib_data = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=r\"D:\\Projects\\IEEE Indicon Intel AI Hackathon\\datasets\\medicine_conversations.json\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "eval_data = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=r\"D:\\Projects\\IEEE Indicon Intel AI Hackathon\\datasets\\medicine_conversations.json\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "from neural_compressor import quantization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model = VisionEncoderDecoderModel.from_pretrained('./trained_trocr_model').to(device)\n",
    "quantized_model = quantization.fit(trained_model, conf, calib_dataloader=calib_data, eval_dataloader=eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "        set_seed,\n",
    "\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "from trl import (\n",
    "   SFTTrainer)\n",
    "\n",
    "# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n",
    "# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "\n",
    "# Anthropic/Vicuna like template without the need for special tokens\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "\n",
    "# ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "\n",
    "    # Tokenizer        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "    \n",
    "    # template dataset\n",
    "    def template_dataset(examples):\n",
    "        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    \n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "\n",
    "    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False,  # No need to add additional separator token\n",
    "        },\n",
    "    )\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    ##########################\n",
    "    # Train model\n",
    "    ##########################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "    trainer.save_model()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = TrlParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_and_config()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    training_function(script_args, training_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
